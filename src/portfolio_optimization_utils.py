"""
Daily Portfolio Optimization using GARCH volatility forecasts and Mean-Variance (Max Sharpe) allocation

Assets: NVDA, PLTR, TSLA (ticker list configurable)

Pipeline steps:
1. Download historical adjusted close prices (yfinance) or load from local CSV.
2. Compute log returns.
3. For each asset, fit a univariate GARCH(1,1) (using arch package) on returns residuals to forecast next-day variance.
4. Build a forecast covariance matrix using the forecasted volatilities and the empirical correlation matrix computed from a recent window.
5. Estimate expected returns (configurable): by default, use exponentially-weighted mean return; you may replace with your own forecast.
6. Run mean-variance optimization to maximize the portfolio Sharpe ratio subject to weights summing to 1 and long-only bounds.
7. Output weights and (optionally) save to CSV. Intended to be run daily (e.g., via cron).

Notes:
- This code requires: yfinance, pandas, numpy, scipy, arch
- Install with: pip install yfinance pandas numpy scipy arch
- The environment must have internet access to download prices. If not, provide a CSV file with columns 'Date' + tickers.

Author: Generated by ChatGPT for Meraj (Data Scientist)
"""

import os
import datetime as dt
import numpy as np
import pandas as pd

# Financial data
try:
    import yfinance as yf
except Exception as e:
    yf = None

# GARCH
from arch import arch_model

# Optimization
from scipy.optimize import minimize


def download_prices(tickers, start_date="2018-01-01", end_date=None):
    """Download adjusted close prices. If yfinance is unavailable, raise an informative error."""
    if end_date is None:
        end_date = dt.date.today().isoformat()
    if yf is None:
        raise ImportError("yfinance not available in this environment. Install yfinance or provide a local CSV with price data.")
    data = yf.download(tickers, start=start_date, end=end_date, progress=False, auto_adjust=True)["Close"]
    # If single ticker returns Series, convert to DataFrame
    if isinstance(data, pd.Series):
        data = data.to_frame()
    data = data.dropna(how="all")
    return data


def compute_log_returns(prices):
    return np.log(prices).diff().dropna()


def fit_garch_and_forecast_var(returns, p=1, q=1, forecast_horizon=1):
    """Fit a GARCH(p,q) model to each column in returns and forecast variance for next `forecast_horizon` days.
    Returns a dict: {ticker: forecast_variance}
    """
    forecast_vars = {}
    for col in returns.columns:
        series = returns[col].dropna() * 100  # scale to percent returns for numerical stability
        try:
            am = arch_model(series, mean='Constant', vol='GARCH', p=p, q=q, dist='normal')
            res = am.fit(disp='off')
            # one-step ahead forecast
            fc = res.forecast(horizon=forecast_horizon, reindex=False)
            # variance forecast (in percent^2) -> convert back to decimal
            var_forecast = fc.variance.values[-1, -1] / (100**2)
            forecast_vars[col] = var_forecast
        except Exception as e:
            # fallback: use sample variance if GARCH fails
            forecast_vars[col] = series.var() / (100**2)
    return forecast_vars


def build_cov_matrix(forecast_vars, returns, corr_window=252):
    """Construct covariance matrix from forecast variances and empirical correlations.

    forecast_vars: dict of var per ticker (decimal)
    returns: DataFrame of returns (decimal)
    corr_window: lookback window (days) to compute correlations
    """
    tickers = list(forecast_vars.keys())
    # compute correlation matrix using recent window
    recent = returns.tail(corr_window)
    corr = recent.corr()
    cov = pd.DataFrame(index=tickers, columns=tickers, dtype=float)
    for i in tickers:
        for j in tickers:
            cov.loc[i, j] = np.sqrt(forecast_vars[i]) * np.sqrt(forecast_vars[j]) * corr.loc[i, j]
    return cov


def estimate_expected_returns(returns, method='ewma', span=60):
    """Estimate expected returns. Default: EWMA of historical returns (not purely historical mean).
    You can replace this with your own return forecasts.
    Returns a pandas Series of expected returns (daily).
    """
    if method == 'ewma':
        # exponentially weighted moving average of daily returns
        mu = returns.ewm(span=span).mean().iloc[-1]
    elif method == 'mean':
        mu = returns.mean()
    else:
        raise ValueError('Unknown method for expected returns')
    return mu


def max_sharpe_weights(expected_returns, cov_matrix, risk_free_rate=0.0, bounds=(0.0, 1.0)):
    """Mean-variance optimizer that maximizes the Sharpe ratio (long-only by default).

    expected_returns: pd.Series of expected daily returns
    cov_matrix: pd.DataFrame covariance matrix (daily)
    """
    tickers = expected_returns.index.tolist()
    n = len(tickers)
    mu = expected_returns.values
    cov = cov_matrix.values.astype(float)

    # convert daily risk-free rate to daily if annual given (assuming risk_free_rate is annual)
    # For simplicity, assume risk_free_rate is in same units as expected_returns (default 0)

    # objective: negative Sharpe = - (w^T mu - rf) / sqrt(w^T cov w)
    def neg_sharpe(w):
        port_ret = np.dot(w, mu)
        port_vol = np.sqrt(w @ cov @ w)
        # protect against zero vol
        if port_vol == 0:
            return 1e6
        return -(port_ret - risk_free_rate) / port_vol

    # constraints: sum(weights) = 1
    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})
    bounds = tuple([bounds] * n)
    x0 = np.repeat(1.0 / n, n)

    res = minimize(neg_sharpe, x0=x0, bounds=bounds, constraints=constraints, method='SLSQP', options={'ftol':1e-9, 'maxiter':1000})

    if not res.success:
        print('Optimization warning:', res.message)
    weights = pd.Series(res.x, index=tickers)
    return weights


def run_daily_pipeline(tickers, start_date='2018-01-01', end_date=None, lookback_corr=252, ewma_span=60, garch_p=1, garch_q=1, save_path='weights_daily.csv'):
    prices = download_prices(tickers, start_date=start_date, end_date=end_date)
    returns = compute_log_returns(prices)

    # forecast variances using GARCH
    print('Fitting GARCH models and forecasting variances...')
    forecast_vars = fit_garch_and_forecast_var(returns, p=garch_p, q=garch_q, forecast_horizon=1)
    print('Forecast variances (next day):', forecast_vars)

    # build covariance matrix
    cov = build_cov_matrix(forecast_vars, returns, corr_window=lookback_corr)

    # expected returns (you may replace with another forecast)
    exp_ret = estimate_expected_returns(returns, method='ewma', span=ewma_span)

    # optimize
    print('Running mean-variance optimization (max Sharpe)...')
    weights = max_sharpe_weights(exp_ret, cov, risk_free_rate=0.0, bounds=(0.0, 1.0))

    # save
    today = dt.date.today().isoformat()
    out = pd.DataFrame({'date': [today], **{t: [w] for t, w in weights.items()}})
    if os.path.exists(save_path):
        prev = pd.read_csv(save_path)
        combined = pd.concat([prev, out], ignore_index=True)
        combined.to_csv(save_path, index=False)
    else:
        out.to_csv(save_path, index=False)

    return weights, cov, exp_ret


if __name__ == '__main__':
    # Example usage for the three-stock portfolio
    tickers = ['NVDA', 'PLTR', 'TSLA']
    weights, cov_matrix, expected_returns = run_daily_pipeline(tickers, start_date='2020-01-01')
    print('\nOptimized weights (long-only, max Sharpe):\n')
    print(weights)
    print('\nExpected returns used (daily):\n')
    print(expected_returns)
    print('\nForecast covariance matrix (daily):\n')
    print(cov_matrix)

    # To run daily automatically: create a cron job or scheduled task that runs this script every trading day before market open.
    # Example cron (server local time):
    # 0 7 * * 1-5 /usr/bin/python3 /path/to/daily_garch_mv_portfolio.py

